---
title: "Final Project!"
author: "Eric Jensen"
date: "April 15, 2017"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages(require(plyr))
suppressPackageStartupMessages(require(dplyr))
suppressPackageStartupMessages(require(class))
suppressPackageStartupMessages(require(gmodels))
suppressPackageStartupMessages(require(stats))
suppressPackageStartupMessages(require(party))
suppressPackageStartupMessages(require(magrittr))
suppressPackageStartupMessages(require(beepr))
suppressPackageStartupMessages(require(caret))
suppressPackageStartupMessages(library(kernlab))
suppressPackageStartupMessages(library(neuralnet))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(ada))
suppressPackageStartupMessages(library(C50))
suppressPackageStartupMessages(library(e1071))


```

#Cleaning up data
```{r}
speed <- read.csv("Speed Dating Data.csv")
speed$condtn <- as.factor(speed$condtn)
speed$gender <- as.factor(speed$gender)
speed$match <- as.factor(speed$match)
speed$field_cd <- as.factor(speed$field_cd)
levels(speed$field_cd) <- c("Law","Math","SocScie/Psych", "MedSci", "Engineering", "English", "History", "Business", "Education", "Bio","SocialWork","Undergrad", "PoliSci", "Film","FineArts","Lang","Architecture","Other")
speed$race <- as.factor(speed$race)
speed$goal <- as.factor(speed$goal)
levels(speed$goal) <- c("FunNightOut", "MeetNewPpl", "GetADate","SRSRelationship", "ToSayIDidIt","Other")
speed$date <- as.factor(speed$date)
levels(speed$date) <- c("SVRL/Week","2/Week","1/Week","2/Month", "1/Month", "SVRL/Year", "AlmostNever")
speed$go_out <- as.factor(speed$go_out)
levels(speed$go_out) <- c("SVRL/Week","2/Week","1/Week","2/Month", "1/Month", "SVRL/Year", "AlmostNever")
speed$career_c <-as.factor(speed$career_c)
levels(speed$career_c) <- c("Lawyer","Academic/Research","Psychologist","DocMed", "Engineer", "Entertainment", "Banking/Consulting", "RealEstate","IntlAffairs","Undecided","SocialWork","SpeechPath","Politics", "ProSports", "Other", "Journalism", "Architecture")
speed$race_o <-as.factor(speed$race_o) 
speed$dec_o <- as.factor(speed$dec_o)
speed$samerace <- as.factor(speed$samerace)

sd2 <- speed
sd2 <- sd2[ , -1] #IID  
sd2 <- sd2[, -1] #ID  
sd2 <- sd2[, -2] #IDG
sd2 <- sd2[, -3] #Wave
sd2 <- sd2[, -3] #Round
sd2 <- sd2[, -3] #Position
sd2 <- sd2[, -3] #Postion1
sd2 <- sd2[, -4] #Partner 
sd2 <- sd2[, -4] #PID
sd2 <- sd2[, -26]#Field
sd2 <- sd2[, -(27:29)]#Academics
sd2 <- sd2[,-(30:32)]#Socioeconomic 
sd2 <- sd2[,-33]#Career
sd2 <- sd2[,-(59:64)]#What others look for
sd2 <- sd2[,-(70:74)]#Others perception
sd2 <- sd2[,-(81:92)]#Data gathered after intitial
sd2 <- sd2[,(1:79)]
sd2 <- sd2[,-(70:79)] #Removes Post First Date
sd2 <- sd2[,-52]#exclude expnum
sd3 <- cbind(sd2, speed[, 6])

sdrandom <- sd2[sample(nrow(sd2), nrow(sd2)),] #Get a random sample since the data is organized by participant
sdrandom1 <-sd3[sample(nrow(sd3), nrow(sd3)),]

sdclean <- na.omit(sdrandom) #Remove rows with NA values to create a "clean" set
sdclean1 <- na.omit(sdrandom1)

names(sdclean1)[69] <- "wave"
sdclean1[, 69] <- as.factor(sdclean1[, 69])

sdclean1$month <- ifelse(sdclean1$wave == 1, "October", ifelse(sdclean1$wave == 2, "October", ifelse(sdclean1$wave == 3, "November", ifelse(sdclean1$wave == 4, "November", ifelse(sdclean1$wave == 5, "November", ifelse(sdclean1$wave == 6, "March", ifelse(sdclean1$wave == 7, "March", ifelse(sdclean1$wave == 8, "April", ifelse(sdclean1$wave == 9, "April", ifelse(sdclean1$wave == 10, "September", ifelse(sdclean1$wave == 11, "September", ifelse(sdclean1$wave == 12, "October", ifelse(sdclean1$wave == 13, "October", ifelse(sdclean1$wave == 14, "October", ifelse(sdclean1$wave == 15, "February", ifelse(sdclean1$wave == 16, "February", ifelse(sdclean1$wave == 17, "February", ifelse(sdclean1$wave == 18, "April", ifelse(sdclean1$wave == 19, "April", ifelse(sdclean1$wave == 20, "April",ifelse(sdclean1$wave == 21, "April", NA
                        )))))))))))))))))))))

#Creating model matrix
data <- as.data.frame(model.matrix(~. -1, data = sdclean))

#removing match1
data$match <- as.factor(data$match1)
data$match1 <- NULL

# Rescale the data
normalize <- function(x) {
  return((x-min(x)) / (max(x) - min(x)))
}

data <-as.data.frame(lapply(data, function(x){
  if((class(x[1]) != "numeric") & (class(x[1]) != "integer")) {
    return (x)
  }
  return(normalize(x))
}))

#setting y as a factor
data$y <- as.factor(data$match)
data$match <- NULL

#splitting data into training and test
train <- data[1:5199, ]
test <- data[5200:6499, ]
```

```{r}
explore <- sdclean1

explore$match <- as.numeric(ifelse(explore$match == 1,1,0))

month.match.ratio <- explore %>% group_by(gender, month) %>% summarise(match.prop = mean(match))

ggplot(month.match.ratio, aes(x = month, y = match.prop, fill = gender)) + geom_col(position = "dodge")+ ylab("Matching Probability") + xlab("Month")

```
Individuals, particularly women, are more likely to match in November than in any other month. This is likely in anticipation of the holiday season. Knowing this, okMatch should look into providing dating rounds in December to take advantage of this season of higher matching.

```{r}
explore$int_corr <- as.factor(ifelse(explore$int_corr < -0.3355, "<.05",
                                        ifelse(explore$int_corr< .1965, " < .5", 
                                        ifelse(explore$int_corr <.655 , ">.5", ">.95"))))

int.match.ratio <- explore %>% group_by(gender, int_corr) %>% summarise(match.prop = mean(match))

ggplot(int.match.ratio, aes(x = int_corr, y = match.prop, fill = gender)) + geom_col(position = "dodge")+ ylab("Matching Probability") + xlab("Level of Interest Correlation")
```

You would think that matching would be proportional to the level of interest correlation but that isn't true! With women in particular, if you have an interest correlation of below 5%, you are still more likely to match than any other group other than an interest correlation of 95% or higher.

#Predate attitude
```{r}
explore$exphappy <- as.factor(ifelse(explore$exphappy < 2, "<.05",
                                        ifelse(explore$exphappy< 5.5, " < .5", 
                                        ifelse(explore$exphappy <8 , ">.5", ">.95"))))

exhappy.match.ratio <- explore %>% group_by(gender, exphappy) %>% summarise(match.prop = mean(match))

ggplot(exhappy.match.ratio, aes(x = exphappy, y = match.prop, fill = gender)) + geom_col(position = "dodge")+ ylab("Matching Probability") + xlab("Level of Expected Happiness")

```

Men who go into a date expected to no be happy with their partners are much more likely to match. This could be an issue of low sample size, or they don't expect anything with their partners so they'll match with anyone. 

#Variable exploration
```{r}
explore <- sdclean
names(explore)

explore$gender <- factor(explore$gender, levels = c(0, 1), labels = c("Female", "Male"))


```

#Matching based on age

Age profile of people who participated in the events
```{r}
summary(explore$age)
ggplot(explore, aes(x=age , fill=gender))+ geom_histogram() + ggtitle("Age Distribution") + xlab("Participant Age Group")

```

Divide age of participants into <25, 25-30, 31-35, 36-40, 41-50, >50
```{r}
explore.age <- explore
explore.age$participant.age.group <- as.factor(ifelse(explore$age<25, "<25",
                                        ifelse(explore$age<31, "25-30", 
                                        ifelse(explore$age<36, "31-35",
                                        ifelse(explore$age<41, "36-40",
                                        ifelse(explore$age<51,"41-50",">50"))))))
#See the distribution of each age group
plot(explore.age$participant.age.group)
table(explore.age$participant.age.group)


#To calculate percentage of matching easier, convert "Yes" and "No" into numerical value 0,1
explore.age$match <- as.numeric(ifelse(explore$match == 1,1,0))

age.match.ratio <- explore.age %>% group_by(gender, participant.age.group) %>% summarise(match.prop = mean(match))

ggplot(age.match.ratio, aes(x = participant.age.group, y = match.prop, fill = gender)) + geom_col(position = "dodge")+ ylab("Matching Probability") + xlab("Participant Age Group")
```

Since there are only male in participant age group 41-50, there is only probability of matching for male sample. Interestingly, this group of male aged 41-50 has the highest probability to match. 1 in 4 of this sample group matched. In the most popular age group, the probability of male and female to match is really close, and male has slightly higher probability to match in group of age less than 25.

#Age correlation of participants and partners
```{r}
#Plot age of participant and age of partner 

age.match <- explore.age[c("age","age_o")]
ggplot(age.match, aes(x = age, y = age_o)) + geom_point()+ geom_smooth(method=lm) +ylab("Age of Partner")+ xlab("Age of Participant")+ggtitle("Age Match Plot")

#Age of participant and age of partner correlation

cor(age.match$age, age.match$age_o)
```

There is not much correlation between age of participants and age of partners.In fact, participants aged 55 matched with partners from age 20-40.

```{r}
explore$match <- as.numeric(ifelse(explore$match == 1,1,0))
race.match.ratio <- explore  %>% group_by(gender, race_o) %>% summarise(match.prop = mean(match))

ggplot(race.match.ratio, aes(x = race_o, y = match.prop, fill = gender)) + geom_col(position = "dodge")+ ylab("Matching Probability") + xlab("Race")

Black <- subset(explore, explore$race == 1)
White <- subset(explore, explore$race == 2)
Latino <- subset(explore, explore$race == 3)
Asian <- subset(explore, explore$race == 4)
Native <- subset(explore, explore$race == 5)
Other <- subset(explore, explore$race == 6)

black.match.ratio <- Black %>% group_by(gender, race_o) %>% summarise(match.prop = mean(match))

ggplot(black.match.ratio, aes(x = race_o, y = match.prop, fill = gender)) + geom_col(position = "dodge")+ ylab("Matching Probability") + xlab("Race")

```
Black people are the most likely to match and are significantly more likely to match with each other.

```{r}
white.match.ratio <- White %>% group_by(gender, race_o) %>% summarise(match.prop = mean(match))

ggplot(white.match.ratio, aes(x = race_o, y = match.prop, fill = gender)) + geom_col(position = "dodge")+ ylab("Matching Probability") + xlab("Race")
```

White males are most likely to match with white women buut white women match with all other races proportionally higher other than asian and native american

```{r}

latino.match.ratio <- Latino %>% group_by(gender, race_o) %>% summarise(match.prop = mean(match))

ggplot(latino.match.ratio, aes(x = race_o, y = match.prop, fill = gender)) + geom_col(position = "dodge")+ ylab("Matching Probability") + xlab("Race")

```

Latino women match proportionally the highest with "other" men and black men. The other men statistic might be an anomaly because of a low sample size. Ignoring, other the same race trend continues. 

```{r}
asian.match.ratio <- Asian %>% group_by(gender, race_o) %>% summarise(match.prop = mean(match))

ggplot(asian.match.ratio, aes(x = race_o, y = match.prop, fill = gender)) + geom_col(position = "dodge")+ ylab("Matching Probability") + xlab("Race")
```

Asians do not match with other Asians at the same race. As appeared with the aggregate, they are most likely to match with black people.

```{r}
career.match.ratio <- explore %>% group_by(gender, career_c ) %>% summarise(match.prop = mean(match))

ggplot(career.match.ratio, aes(x = career_c, y = match.prop, fill = gender)) + geom_col(position = "dodge")+ ylab("Matching Probability") + xlab("Participant Career Group") + theme(axis.text.x = element_text(angle = 90, hjust=1))
```


#Logistic Models
```{r}
library(caret)
## 10-fold CV
fitControl <- trainControl(method = "cv",
                           number = 10,
                           selectionFunction = "oneSE",
                           verboseIter = T)


# ===================================================
# Logistic Models
# ===================================================

run_all_log_models = F

if (run_all_log_models) {
  log <- glm(y~., train, family = "binomial") #used binomial bc works well for binary outcomes and continuous predictors
  #Stepped forward model
  log_f <- colnames(train) %>%
  {paste(.[! . %in% "y"], collapse = " + ")} %>%
    paste("y~ ", .) %>% as.formula()
  log_stepped <-glm(y~1, train, family = "binomial")
  log_stepped <- step(log_stepped, scope = (log_f), direction = "forward")
  #Boosted logistic Model
  log_boosted <- caret::train(y ~ . , data = train,
                              method = "LogitBoost",
                              trcontrol = fitControl,
                              metric = "Accuracy")
  
  save(log, file = "log.txt")
  save(log_stepped, file = "log_stepped.txt")
  save(log_boosted, file = "log_boosted.txt") 
} else {
  load(file = "log.txt")
  load(file = "log_stepped.txt")
  load(file = "log_boosted.txt") 
}

```

#SVM
```{r}
#========================
# SVM Models
#========================

library(kernlab)

run_all_svm_models = F

if (run_all_svm_models) {
  svm_model1 <- ksvm(y~., data = train, kernel = "vanilladot")
  svm_model2 <- ksvm(y ~ ., data = train, kernel = "rbfdot")

  # #Bagged svm did not work, "said something is wrong"
  # bagctrl <- bagControl(fit = svmBag$fit, predict = svmBag$pred, aggregate = svmBag$aggregate)
  # set.seed(200)
  # svmbag <- train(y ~ ., data = train, "bag",trControl = ctrl, bagControl = bagctrl)
  
  save(svm_model1, file = "svm_model1.txt")
  save(svm_model2, file = "svm_model2.txt")
} else {
  load(file = "svm_model1.txt")
  load(file = "svm_model2.txt")
}
```

#KNN

```{r}
#===================
# KNN Models
#===================

#Setting up the data
knn_train_labels <- train[,120 ]
knn_test_labels <- test[, 120]

run_all_knn_models = F

if (run_all_knn_models) {
  knn_pred <- knn(train = train, test = test, cl = knn_train_labels, k = 3)
  set.seed(400)
  ctrl <- trainControl(method="repeatedcv",repeats = 3) 
  knnFit <- train(y ~ ., data = train, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)
  #K of 15 ended up being used
  
  #Save the models
  save(knn_pred, file = "knn_pred.txt")
  save(knnFit, file = "knnFit.txt")
} else {
  load(file = "knn_pred.txt")
  load(file = "knnFit.txt")
} 

knnFit
#Predict on tuned model
knnPredict <- predict(knnFit,newdata = test)
```

#Neural Networks

```{r}
#==============================
# Neural Networks
#==============================
library(neuralnet)

nndata <- as.data.frame(model.matrix(~. +0, data = data))
nndata$y0 <- NULL
nndata$y <- nndata$y1
nndata$y1 <- NULL

trainNN <- nndata[1:5199, ]
testNN <- nndata[5200:6499, ]

n <- names(nndata)
#Create Formula for all names in the dataframe
f <- as.formula(paste("y ~", paste(n[!n %in% "y"], collapse = " + ")))

run_all_nn_models = F

if (run_all_nn_models) {
  nn1 <- neuralnet(f,trainNN, hidden = 1)
  nn3 <- neuralnet(f,trainNN, hidden = 3)
  
  #Save the models
  save(nn1, file = "neural_model_1.txt")
  save(nn3, file = "neural_model_3.txt")
} else {
  load(file = "neural_model_1.txt")
  load(file = "neural_model_3.txt")
}
```


#Tree Models
```{r}
#==============================
# Tree based models
#==============================
library(party)
library(randomForest)
library(ada)
library(dplyr)
library(class)

run_all_tree_models = F

if(run_all_tree_models){
  tree_model <- ctree(y ~ ., data = train, 
                      control = ctree_control(mincriterion = .99))
  tree_random_forest <- randomForest(y~., data = train)
  # Ada Tree
  tree_ada <- ada(y~., data = train)
  #Modify forests
  ctrl <- trainControl(method = "repeatedcv",
                     number = 10, repeats = 10)
# auto-tune a random forest
  grid_rf <- expand.grid(.mtry = c(2, 4, 8, 16))
  set.seed(300)
  #Modified Random Forest
  m_rf <- train(y ~ ., data = train, method = "rf",
              metric = "Kappa", trControl = ctrl,
              tuneGrid = grid_rf)
  
  save(m_rf, file = "modified_random_forest.txt")
  save(tree_model, file = "tree_model.txt")
  save(tree_random_forest, file = "tree_random_forest.txt")
  save(tree_ada, file = "tree_ada.txt")
} else {
  load(file = "modified_random_forest.txt")
  load(file = "tree_model.txt")
  load(file = "tree_random_forest.txt")
  load(file = "tree_ada.txt")
}
```

#Predictions
```{r}
#Logistic
log_prediction <- predict(log, test, type = "response") %>%
{ifelse(. > 0.5, "1", "0")} %>% as.factor()
log_stepped_prediciton <- predict(log_stepped, test, type = "response")%>%
{ifelse(. > 0.5, "1", "0")} %>% as.factor()
log_boosted_prediction <- predict(log_boosted, test, type = "raw")

#Neural Networks
neuralnet_prediction1 <- neuralnet::compute(nn1, testNN[,1:119])$net.result %>% {ifelse(. > 0.5, "1", "0")} %>% as.factor()
neuralnet_prediction3 <- neuralnet::compute(nn3, testNN[,1:119])$net.result %>% {ifelse(. > 0.5, "1", "0")} %>% as.factor()

#Tree Based
tree_prediction <- predict(tree_model, test)
tree_random_forest_prediction <- predict(tree_random_forest, test)
tree_ada_prediction <- predict(tree_ada, test)
tree_m_rf_prediction <- predict(m_rf, test)
```

#Checking accuracy
```{r}
accuracy <- function(predicted, trueval, model, hideoutput = F) {
  stopifnot(length(predicted) == length(trueval))
  result <- sum(predicted == trueval) / length(predicted)
  if (!hideoutput) {cat("Model:", model, "had", result, "accuracy\n")}
  return(result)
}
a1 <-accuracy(knnPredict, test$y, "KNN (15)", T)
a2 <-accuracy(knn_pred, test$y, "KNN (3)", T)
a3 <-accuracy(log_prediction, test$y, "Log Prediction", T)
a4 <-accuracy(log_stepped_prediciton, test$y, "Log Stepped", T)
a5 <- accuracy(log_boosted_prediction, test$y, "Log Boosted", T)
a6 <- accuracy(neuralnet_prediction1, testNN$y, "Neural Net (1 node)", TRUE)
a7 <- accuracy(neuralnet_prediction3, testNN$y, "Neural Net (3 nodes)", TRUE)
a8 = accuracy(tree_prediction, test$y, "CTree Regression", TRUE)
a9 = accuracy(tree_random_forest_prediction, test$y, "Random Forest Classification", TRUE)
a10 = accuracy(tree_ada_prediction, test$y, "Ada Boost Classification", TRUE)
a11 = accuracy(tree_m_rf_prediction, test$y, "Modified Random Forest", TRUE)

acc_predictions = c(a1, a2, a3, a4, a5, a6, a7, a8,a9,a10, a11)
names = c("Tuned Knn (15)", "KNN (3)", "Log Prediction","Log Stepped", "Log Boosted", "Neural Net (1node)", "Neural Net (3nodes)", "CTree Regression","Random Forest Classification","Ada Boost Classification", "Modified Random Forest")

dotchart(acc_predictions, labels = names, main = "Accuracy of the models", xlab = "Accuracy")
```


#Stacked Model
This combines all the other previous models

```{r}
# ==========================================================
# Get the various predictions for the train data
# ==========================================================

knn_pred_train <- knn(train = train, test = train, cl = knn_train_labels, k = 3)
knnPredict_train <- predict(knnFit,newdata = train)
log_prediction_train <- predict(log, train, type = "response") %>%
{ifelse(. > 0.5, "1", "0")} %>% as.factor()
log_stepped_prediction_train <- predict(log_stepped, train, type = "response")%>%
{ifelse(. > 0.5, "1", "0")} %>% as.factor()
log_boosted_prediction_train <- predict(log_boosted, train, type = "raw")
neuralnet_prediction1_train <- neuralnet::compute(nn1, trainNN[,1:119])$net.result %>% {ifelse(. > 0.5, "1", "0")} %>% as.factor()
neuralnet_prediction3_train <- neuralnet::compute(nn3, trainNN[,1:119])$net.result %>% {ifelse(. > 0.5, "1", "0")} %>% as.factor()
tree_prediction_train <- predict(tree_model, train)
tree_random_forest_prediction_train <- predict(tree_random_forest, train)
tree_ada_prediction_train <- predict(tree_ada, train)
tree_m_rf_prediction_train <- predict(m_rf, train)

#Combined Model
model_combined_results <- data.frame(log_prediction_train, log_boosted_prediction_train, log_stepped_prediction_train, knn_pred_train, knnPredict_train, neuralnet_prediction1_train, neuralnet_prediction3_train, tree_prediction_train, tree_random_forest_prediction_train, tree_ada_prediction_train, tree_m_rf_prediction_train) %>% as.tbl()

# Convert all of the feature data to factors
ConvertToYesNo <- function(myprediction) {
  result <- myprediction %>% as.factor()
  levels(result) <- c("0", "1")
  result
}

# Convert each set of predictions to factors
knnPredict %<>% ConvertToYesNo()
knn_pred %<>% ConvertToYesNo()
log_prediction %<>% ConvertToYesNo()
log_boosted_prediction %<>% ConvertToYesNo()
log_stepped_prediciton %<>% ConvertToYesNo()
neuralnet_prediction1 %<>% ConvertToYesNo()
neuralnet_prediction3 %<>% ConvertToYesNo()
tree_prediction %<>% ConvertToYesNo()
tree_random_forest_prediction %<>% ConvertToYesNo()
tree_ada_prediction %<>% ConvertToYesNo()
tree_m_rf_prediction %<>% ConvertToYesNo()

stacked_data <- data.frame(log_prediction, log_boosted_prediction, log_stepped_prediciton, knn_pred, knnPredict, neuralnet_prediction1, neuralnet_prediction3, tree_prediction, tree_random_forest_prediction, tree_ada_prediction, tree_m_rf_prediction) %>% as.tbl()

# Swap out the names
names(model_combined_results) <- names(stacked_data)

#Stacked Ctree model
stacked_model <- ctree(train$y ~ . + 1, data = model_combined_results, controls = ctree_control(mincriterion = .95)) %T>% plot

stacked_model_prediction = predict(stacked_model, stacked_data, type = "response")
# Convert the levels into something that the factors can compare
stacked_model_prediction <- ConvertToYesNo(stacked_model_prediction)
accuracy(stacked_model_prediction, test$y, "Stacked Model Accuracy")

#Good model can talk about how the low knn is helping the model
plot(stacked_model)

```
From the stacked model, we can see that tree_m_rf_prediction from the tree random forest model mostly made a call whether the couple is going to match or not. However, KNN model will becomes a decision call when tree_m_rf_prediction predict no and KNN predict yes. In this case, the stacked model predict that the couple is going to match. 


##Cost model 

#Cost model in a start-up stage

We further improve the decision tree model by assigning the cost matrix. As this okMatch is a startup business, we would want to set the primary goal to match as many people as possible. In other words, we can accept some positive error (the bad dates), that is, the couple is not actually match, but our platform predicts that they are going to match, if that means we can generate the most match possible (true positive).

```{r}
#decision tree
#Determine lots of matching to maximum no. of dates (False positive) VS determine lots of corrected matching on the expense of some possible match missed (False negative)
#startup vs match.com
#Adding Error Costs

#This increases total number of matches, flip matrix to be more selective on matches
error_cost_max_match <- matrix(c(0, 1, 20, 0), nrow = 2)

#Cost Model maximum the number of matching as an expense with false positive
cost_stacked_model_max_match <- C5.0(train$y ~ . + 1, data = model_combined_results, costs = error_cost_max_match)

#Prediction
cost_model_prediction_max_match = predict(cost_stacked_model_max_match, stacked_data, type = "class")

accuracy(cost_model_prediction_max_match, test$y, "Stacked Model Accuracy")

#comparing to previous model
confusionMatrix(cost_model_prediction_max_match, test$y)
confusionMatrix(stacked_model_prediction, test$y)
```

Accuracy of cost_model_prediction_max_match decrease, but the True Positive (Number of match that we predicted correctly) increase. 

Let's see what happens if we change the weight in the cost matrix

```{r}
#This increases total number of matches, flip matrix to be more selective on matches
error_cost_max_match2 <- matrix(c(0, 30, 1, 0), nrow = 2)

#Cost Model maximum the number of matching as an expense with false positive
cost_stacked_model_max_match2 <- C5.0(train$y ~ . + 1, data = model_combined_results, costs = error_cost_max_match2)

#Prediction
cost_model_prediction_max_match2 = predict(cost_stacked_model_max_match2, stacked_data, type = "class")

accuracy(cost_model_prediction_max_match2, test$y, "Stacked Model2 Accuracy")

#comparing to previous model

confusionMatrix(cost_model_prediction_max_match2, test$y)
confusionMatrix(cost_model_prediction_max_match, test$y)
confusionMatrix(stacked_model_prediction, test$y)

```

Using cost_model_prediction_max_match2, in which the cost to misclassify the couple as not match is 30 times the cost that we misclassify the couple as match, the case of false negative reduced to 8, and the case of true positive increases to 225. However, the false positive case increase almost two times from the cost_model_prediction_max_match. We deemed that this is not a beneficial trade-off and recommend clients to use cost_model_prediction_max_match in their start-up stage. 

#Cost model in a mature stage

When the platform has been recognized, and client can build an extensive customer base, then we start to consider shifting the weight in cost matrix. This time, we want our platform to be reliable in generating "good dates". For example, they might want people to find a good dates easier, as opposed to going through a lot of people in that evening to find a date. In other words, we want to be more accurate on our matching prediction with expense of missing some actual matches. We want to minimize the false positive.

```{r}
#This assign 3 times of cost on false positive and 1 times on false negative. 
error_cost_min_FP <- matrix(c(0, 3, 1, 0), nrow = 2)

#Cost Model minimize the false positive with the expense of increasing false negative
cost_stacked_min_FP <- C5.0(train$y ~ . + 1, data = model_combined_results, costs = error_cost_min_FP)

#Prediction
cost_model_prediction_min_FP = predict(cost_stacked_min_FP, stacked_data, type = "class")

accuracy(cost_model_prediction_min_FP, test$y, "Stacked Model3 Accuracy")

#comparing to previous model

confusionMatrix(cost_model_prediction_min_FP, test$y)
confusionMatrix(cost_model_prediction_max_match, test$y)
confusionMatrix(stacked_model_prediction, test$y)

```
Assigning cost of misclassifying the  actual unmatched to matched as 3 times of misclassifying the actual matched to unmatched, we have the accuracy of 0.96, and Kappa of 0.8563 which is an improvement from the previous two models. It reduces the false positive by approximately 10 times, but increase the false negative by approximately 2 times from the other two models. 
We recommend cost_model_prediction_min_FP if our client want to generate reputation of creating an event that people can find their match easier, not to generate the most matching in one night. 
